{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Dict, Any\n",
    "from enum import Enum\n",
    "import importlib\n",
    "\n",
    "# Step 1: Lazy Loading Helper Function\n",
    "def lazy_load(module_name: str, class_name: str):\n",
    "    try:\n",
    "        # Dynamically import the module\n",
    "        module = importlib.import_module(module_name)\n",
    "        # Get the class from the module\n",
    "        return getattr(module, class_name)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading {class_name} from module {module_name}: {e}\")\n",
    "\n",
    "# Step 2: Enum Class for LLM Types\n",
    "class LLM(str, Enum):\n",
    "    OPENAI = \"openai\"\n",
    "    AZURE_OPENAI = \"azure_openai\"\n",
    "    HUGGINGFACE = \"huggingface\"\n",
    "    OLLAMA = \"ollama\"\n",
    "    COHERE = \"cohere\"\n",
    "    VERTEXAI = \"vertexai\"\n",
    "    BEDROCK = \"bedrock\"\n",
    "    JINA = \"jina\"\n",
    "    CUSTOM = \"custom\"\n",
    "\n",
    "# Step 3: Map LLM Types to Lazy-loaded Embedding Classes\n",
    "LLM_MAP = {\n",
    "    LLM.OPENAI: lazy_load(\"langchain_openai\", \"ChatOpenAI\"),\n",
    "    LLM.AZURE_OPENAI: lazy_load(\"langchain_openai\", \"AzureChatOpenAI\"),\n",
    "}\n",
    "\n",
    "# Step 4: Define the LLM Configuration Model\n",
    "class LLMConfig(BaseModel):\n",
    "    model_config = {\"protected_namespaces\": ()}\n",
    "    \n",
    "    type: LLM  # Enum to specify the LLM\n",
    "    model_kwargs: Optional[Dict[str, Any]] = Field(default_factory=dict, description=\"Model-specific parameters like model name/type\")\n",
    "    custom_class: Optional[str] = None  # Optional: If using a custom class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "from pydantic_v1 import Field, BaseModel\n",
    "\n",
    "\n",
    "class BaseConfig(BaseModel):\n",
    "    \"\"\"Base configuration shared across all RAG modules\"\"\"\n",
    "    # input_source: Union[str, List[str]] = Field(..., description=\"File path, directory path, or URL for input data\")\n",
    "    # test_dataset: str = Field(..., description=\"Path to CSV file containing test questions\")\n",
    "    \n",
    "    @classmethod\n",
    "    def from_yaml(cls, file_path: str) -> \"GenerationOptionsConfig\":\n",
    "        with open(file_path, \"r\") as yaml_file:\n",
    "            config = yaml.safe_load(yaml_file)\n",
    "        return cls(**config[\"Generation\"])\n",
    "\n",
    "    def to_yaml(self, file_path: str) -> None:\n",
    "        \"\"\"Save configuration to a YAML file.\"\"\"\n",
    "        with open(file_path, 'w') as file:\n",
    "            yaml.dump(self.model_dump(), file)\n",
    "\n",
    "# Step 2: Define Pydantic Model for Individual LLM Configuration\n",
    "class GenerationConfig(BaseConfig):\n",
    "    type: LLM  # Specifies the LLM type\n",
    "    model_kwargs: Optional[Dict[str, Any]] = Field(default_factory=dict, description=\"Model-specific parameters\")\n",
    "    prompt_template: Optional[str] = None\n",
    "\n",
    "# Step 3: Define Pydantic Model for Overall Generation Configuration\n",
    "class GenerationOptionsConfig(BaseConfig):\n",
    "    llms: List[GenerationConfig]  # List of LLM configurations\n",
    "    prompt_template_path: Optional[str] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashwinaravind/.pyenv/versions/3.12.5/lib/python3.12/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_kwargs\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "import requests\n",
    "from ragbuilder.generation.config import PromptTemplate\n",
    "import pandas as pd\n",
    "def load_prompts(file_name: str = \"rag_prompts.yaml\", url: str= os.getenv(\"RAG_PROMPT_URL\"),read_local: bool = False):\n",
    "    \"\"\"\n",
    "    Load YAML prompts either from a local file or an online source.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): Name of the YAML file. Defaults to \"rag_prompts.yaml\".\n",
    "        read_local (bool): If True, read from a local file. Otherwise, fetch from an online URL.\n",
    "\n",
    "    Returns:\n",
    "        List[PromptTemplate]: A list of PromptTemplate objects.\n",
    "    \"\"\"\n",
    "    yaml_content = None\n",
    "\n",
    "    if read_local:\n",
    "        # Attempt to read from the local file\n",
    "        if os.path.exists(file_name):\n",
    "            print(f\"Loading prompts from local file: {file_name}\")\n",
    "            with open(file_name, 'r') as f:\n",
    "                yaml_content = f.read()\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Local file not found: {file_name}\")\n",
    "    else:\n",
    "        # Attempt to fetch from an online source\n",
    "        print(f\"Fetching prompts from online file: {url}\")\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Raise an HTTP error for bad responses\n",
    "            yaml_content = response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise RuntimeError(f\"Failed to load prompts from URL {url}: {e}\")\n",
    "\n",
    "    # Parse the YAML content\n",
    "    try:\n",
    "        prompts_data = yaml.safe_load(yaml_content)\n",
    "    except yaml.YAMLError as e:\n",
    "        raise ValueError(f\"Failed to parse YAML content: {e}\")\n",
    "\n",
    "    # Convert YAML entries into PromptTemplate objects\n",
    "    prompts = [\n",
    "        PromptTemplate(name=entry['name'], template=entry['template'])\n",
    "        for entry in prompts_data\n",
    "    ]\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from ragbuilder.generation.evaluation import RAGASEvaluator\n",
    "def sample_retriever():\n",
    "    print(\"rag_get_retriever initiated\")\n",
    "    try:\n",
    "        def format_docs(docs):\n",
    "            return \"\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "        # LLM setup\n",
    "        llm = AzureChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "        # Document loader\n",
    "        loader = WebBaseLoader(\"https://raw.githubusercontent.com/ashwinaravind/ashwinaravind.github.io/refs/heads/main/thevanishingtown\")\n",
    "        docs = loader.load()\n",
    "\n",
    "        # Embedding model\n",
    "        embedding = AzureOpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "        # Text splitting and embedding storage\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "        splits = splitter.split_documents(docs)\n",
    "\n",
    "        # Initialize Chroma database\n",
    "        c = Chroma.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=embedding,\n",
    "            collection_name=\"testindex-ragbuilder-retreiver\",\n",
    "            client_settings=chromadb.config.Settings(allow_reset=True),\n",
    "        )\n",
    "\n",
    "        # Retriever setup\n",
    "        retriever = c.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "        ensemble_retriever = EnsembleRetriever(retrievers=[retriever])\n",
    "        print(\"rag_get_retriever completed\")\n",
    "        return ensemble_retriever\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "# sample_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Load YAML File and Parse Configurations\n",
    "from typing import List, Dict, Type\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "class SystemPromptGenerator:\n",
    "    def __init__(self, config: GenerationOptionsConfig, evaluator_class: Type):\n",
    "        self.llms = []  # List to store instantiated LLMs\n",
    "        self.config = config\n",
    "        print(\"printing config\",config)\n",
    "        # self.evaluator = evaluator_class() \n",
    "        self.retriever=sample_retriever\n",
    "        if config.prompt_template_path:\n",
    "            self.prompt_templates = load_prompts(config.prompt_template_path)\n",
    "        else:\n",
    "            self.prompt_templates = load_prompts()\n",
    "        # for llm_config in config.llms:\n",
    "        #     llm_class = LLM_MAP[llm_config.type]  # Get the corresponding LLM class)\n",
    "    def _build_trial_config(self) -> List[GenerationConfig]:\n",
    "            \"\"\"\n",
    "            Build a list of GenerationConfig objects from the provided GenerationOptionsConfig.\n",
    "\n",
    "            Args:\n",
    "                options_config (GenerationOptionsConfig): The input configuration for trial generation.\n",
    "\n",
    "            Returns:\n",
    "                List[GenerationConfig]: A list of generated configurations for trials.\n",
    "            \"\"\"\n",
    "            trial_configs = []\n",
    "            for llm_config in self.config.llms:\n",
    "                # llm_class = LLM_MAP[llm_config.type]  # Get the corresponding LLM class)\n",
    "                llm_instance = LLMConfig(type=llm_config.type, model_kwargs=llm_config.model_kwargs)\n",
    "                llm_class = LLM_MAP[llm_config.type]\n",
    "                # Step 8: Instantiate the Model with the Configured Parameters\n",
    "                llm = llm_class(**llm_config.model_kwargs)\n",
    "                # print(llm_config.type,llm_config.model_kwargs,llm.invoke(\"what is the capital of France?\"))\n",
    "                # print(self.prompt_templates)\n",
    "                for prompt_template in self.prompt_templates:\n",
    "                    trial_config = GenerationConfig(\n",
    "                        type=llm_config.type,  # Pass the LLMConfig instance here\n",
    "                        model_kwargs=llm_config.model_kwargs,\n",
    "                        # evaluator=self.evaluator,\n",
    "                        # retriever=self.retriever,\n",
    "                        # eval_data_set_path=self.config.eval_data_set_path,\n",
    "                        prompt_template=prompt_template.template,\n",
    "                        # read_local_only=self.config.read_local_only,\n",
    "                        )\n",
    "                    # res=self._create_pipeline(trial_config,self.retriever()).invoke(\"Who is Clara?\")\n",
    "                    # print(res)\n",
    "                    trial_configs.append(trial_config)\n",
    "                    break\n",
    "\n",
    "                # print(trial_config)\n",
    "                # trial_configs.append(trial_config)\n",
    "\n",
    "            # print(trial_configs)\n",
    "            # for llm_config in options_config.llms:\n",
    "            #     print(llm_config)\n",
    "            #     trial_config = GenerationConfig(\n",
    "            #         llm_type=llm_config.type,\n",
    "            #         llm_model_kwargs=llm_config.model_kwargs,\n",
    "            #         evaluator=options_config.evaluator,\n",
    "            #         retriever=options_config.retriever,\n",
    "            #         eval_data_set_path=options_config.eval_data_set_path,\n",
    "            #         prompt_template_path=options_config.prompt_template_path,\n",
    "            #         read_local_only=options_config.read_local_only,\n",
    "            #     )\n",
    "            #     trial_configs.append(trial_config)\n",
    "            return trial_configs\n",
    "    def _create_pipeline(self, trial_config: GenerationConfig, retriever: RunnableParallel):\n",
    "        try:\n",
    "            def format_docs(docs):\n",
    "                return \"\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "            # Prompt setup\n",
    "            llm_class = LLM_MAP[trial_config.type]\n",
    "                # Step 8: Instantiate the Model with the Configured Parameters\n",
    "            llm = llm_class(**trial_config.model_kwargs)\n",
    "            prompt_template = trial_config.prompt_template\n",
    "            print('prompt_template',prompt_template)\n",
    "            print(\"testing retriever\\n\",retriever.invoke(\"Who is Clara?\"))\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\", prompt_template),\n",
    "                    (\"user\", \"{question}\"),\n",
    "                    MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # RAG Chain setup\n",
    "            rag_chain = (\n",
    "                RunnableParallel(context=retriever, question=RunnablePassthrough())\n",
    "                .assign(context=itemgetter(\"context\") | RunnableLambda(format_docs))\n",
    "                .assign(answer=prompt | llm | StrOutputParser())\n",
    "                .pick([\"answer\", \"context\"])\n",
    "            )\n",
    "            print(\"rag_pipeline completed\")\n",
    "            return rag_chain\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    def optimize(self):\n",
    "        trial_configs = self._build_trial_config()\n",
    "        print(\"trial_configs\",trial_configs)\n",
    "        pipeline=None\n",
    "        results = {}\n",
    "        evaluator = RAGASEvaluator()\n",
    "        evaldataset=evaluator.get_eval_dataset('/Users/ashwinaravind/Desktop/kruxgitrepo/ragbuilder/gensimtest.csv')\n",
    "        for trial_config in trial_configs:\n",
    "            pipeline = self._create_pipeline(trial_config,self.retriever())\n",
    "            for entry in evaldataset:\n",
    "                question = entry.get(\"question\", \"\")\n",
    "                result=pipeline.invoke(question)\n",
    "                results[trial_config.prompt_template] = []\n",
    "                results[trial_config.prompt_template].append({\n",
    "                        \"prompt_key\": trial_config.prompt_template,\n",
    "                        \"prompt\": trial_config.prompt_template,\n",
    "                        \"question\": question,\n",
    "                        \"answer\": result.get(\"answer\", \"Error\"),\n",
    "                        \"context\": result.get(\"context\", \"Error\"),\n",
    "                        \"ground_truth\": entry.get(\"ground_truth\", \"\"),\n",
    "                        \"config\": trial_config.dict(),\n",
    "                    })\n",
    "                break\n",
    "        output_data = []\n",
    "        for prompt_key, prompt_results in results.items():\n",
    "            output_data.extend(prompt_results)\n",
    "\n",
    "        # Convert to a Dataset directly\n",
    "        from datasets import Dataset\n",
    "        results_dataset = Dataset.from_list(output_data)\n",
    "\n",
    "        # Optionally clean up or format the dataset\n",
    "        if \"context\" in results_dataset.column_names:\n",
    "            results_dataset = results_dataset.map(\n",
    "                lambda x: {\n",
    "                    **x,\n",
    "                    \"contexts\": eval(x[\"context\"]) if isinstance(x[\"context\"], str) and x[\"context\"].startswith(\"[\") else [x[\"context\"]],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        print(\"test_prompt completed\")\n",
    "        # return results_dataset\n",
    "        eval_results=evaluator.evaluate(results_dataset)\n",
    "        # print('eval_results',GenerationConfig(**eval_results['config'][0]))\n",
    "        final_results=self.calculate_metrics(eval_results)\n",
    "        return final_results\n",
    "    # def calculate_metrics(self, result):\n",
    "    #     # Convert the results to a pandas DataFrame\n",
    "    #     print(\"writing config\",result['config'])\n",
    "    #     results_df = result.to_pandas()\n",
    "        \n",
    "    #     # Calculate average correctness per prompt key\n",
    "    #     average_correctness = results_df.groupby(['prompt_key','prompt'])['answer_correctness',].mean().reset_index()\n",
    "    #     average_correctness.columns = ['prompt_key', \"prompt\", 'average_correctness']\n",
    "        \n",
    "    #     # Save the average correctness to a CSV file\n",
    "    #     average_correctness.to_csv('rag_average_correctness.csv', index=False)\n",
    "    #     print(\"The average correctness results have been saved to 'rag_average_correctness.csv'\")\n",
    "        \n",
    "    #     # Find the row with the highest average correctness\n",
    "    #     best_prompt_row = average_correctness.loc[average_correctness['average_correctness'].idxmax()]\n",
    "        \n",
    "    #     # Extract prompt_key, prompt, and average_correctness\n",
    "    #     prompt_key = best_prompt_row['prompt_key']\n",
    "    #     prompt = best_prompt_row['prompt']\n",
    "    #     max_average_correctness = best_prompt_row['average_correctness']\n",
    "    #     config=best_prompt_row['config']\n",
    "    #     return prompt_key, prompt, max_average_correctness,config\n",
    "    #     return config\n",
    "    def calculate_metrics(self, result):\n",
    "    # Convert the results to a pandas DataFrame\n",
    "        results_df = result.to_pandas()\n",
    "\n",
    "        # Group by `prompt_key` and calculate average correctness, while retaining `prompt` and `config`\n",
    "        grouped_results = (\n",
    "            results_df.groupby('prompt_key')\n",
    "            .agg(\n",
    "                prompt=('prompt', 'first'),  # Take the first prompt for each prompt_key\n",
    "                config=('config', 'first'),  # Take the first config for each prompt_key\n",
    "                average_correctness=('answer_correctness', 'mean')  # Calculate average correctness\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        # Save the results to a CSV file\n",
    "        grouped_results.to_csv('rag_average_correctness.csv', index=False)\n",
    "        print(\"The average correctness results have been saved to 'rag_average_correctness.csv'\")\n",
    "\n",
    "        # Find the row with the highest average correctness\n",
    "        best_prompt_row = grouped_results.loc[grouped_results['average_correctness'].idxmax()]\n",
    "\n",
    "        # Extract prompt_key, prompt, average_correctness, and config\n",
    "        prompt_key = best_prompt_row['prompt_key']\n",
    "        prompt = best_prompt_row['prompt']\n",
    "        max_average_correctness = best_prompt_row['average_correctness']\n",
    "        config = best_prompt_row['config']\n",
    "\n",
    "        return prompt_key, prompt, max_average_correctness, config\n",
    "\n",
    "        # return prompt_key, prompt, max_average_correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing config llms=[GenerationConfig(type=<LLM.AZURE_OPENAI: 'azure_openai'>, model_kwargs={'model_name': 'gpt-4o-mini', 'temperature': 0.6}, prompt_template=None)] prompt_template_path=None\n",
      "Fetching prompts from online file: https://raw.githubusercontent.com/ashwinaravind/rag_prompts/refs/heads/main/rag_prompts.yml\n",
      "trial_configs [GenerationConfig(type=<LLM.AZURE_OPENAI: 'azure_openai'>, model_kwargs={'model_name': 'gpt-4o-mini', 'temperature': 0.6}, prompt_template='You are a helpful assistant. Answer any questions solely based on the context provided below. \\nIf the provided context does not have the relevant facts to answer the question, say \"I don\\'t know.\"\\n\\n<context>\\n{context}\\n</context>\\n')]\n",
      "RAGASEvaluator initiated\n",
      "rag_get_retriever initiated\n",
      "rag_get_retriever completed\n",
      "prompt_template You are a helpful assistant. Answer any questions solely based on the context provided below. \n",
      "If the provided context does not have the relevant facts to answer the question, say \"I don't know.\"\n",
      "\n",
      "<context>\n",
      "{context}\n",
      "</context>\n",
      "\n",
      "testing retriever\n",
      " [Document(metadata={'source': 'https://raw.githubusercontent.com/ashwinaravind/ashwinaravind.github.io/refs/heads/main/thevanishingtown'}, page_content='A young woman.\\n\\n---\\n\\n## Chunk 3: The Locket’s Secret\\n\\nClara awoke to a sound—footsteps. Her eyes shot open. In the dim light, she saw a man standing at the door, silhouetted against the night. His green eyes gleamed in the shadows.\\n\\n“Who are you?” she stammered, clutching her locket.\\n\\n“I could ask you the same,” the man replied, stepping forward. “This is my family’s cabin. What are you doing here?”'), Document(metadata={'source': 'https://raw.githubusercontent.com/ashwinaravind/ashwinaravind.github.io/refs/heads/main/thevanishingtown'}, page_content='“Who are you?” she stammered, clutching her locket.\\n\\n“I could ask you the same,” the man replied, stepping forward. “This is my family’s cabin. What are you doing here?”\\n\\nClara’s mind raced. Family? She looked him over. There was something familiar about him, though she was certain they had never met. But those eyes—they felt as though they belonged in her memories, as if they had been watching her all her life.'), Document(metadata={'source': 'https://raw.githubusercontent.com/ashwinaravind/ashwinaravind.github.io/refs/heads/main/thevanishingtown'}, page_content='Just before dusk, Clara stumbled upon a small clearing. In the center stood a dilapidated cabin. Exhausted and with nowhere else to go, she approached. The door creaked open with the softest push, revealing a dusty, abandoned space. She slumped down against the cold stone hearth, staring blankly at the flickering fire she had somehow managed to start.'), Document(metadata={'source': 'https://raw.githubusercontent.com/ashwinaravind/ashwinaravind.github.io/refs/heads/main/thevanishingtown'}, page_content='Before she could respond, the man’s gaze fell on the locket around her neck. His expression changed. “Where did you get that?” he asked sharply.\\n\\n“My mother gave it to me,” Clara said, her fingers instinctively curling around the silver chain. “It’s all I have left of her.”\\n\\nThe man took another step closer, eyes fixed on the locket. “That was my mother’s,” he said softly. “She vanished years ago, wearing that locket.”'), Document(metadata={'source': 'https://raw.githubusercontent.com/ashwinaravind/ashwinaravind.github.io/refs/heads/main/thevanishingtown'}, page_content='The man took another step closer, eyes fixed on the locket. “That was my mother’s,” he said softly. “She vanished years ago, wearing that locket.”\\n\\nClara’s pulse quickened. “That’s impossible. My mother disappeared too—ten years ago.”\\n\\nFor a long moment, neither spoke. Then, without warning, the locket around Clara’s neck grew warm. The clasp that had been locked for years suddenly gave way, and the locket fell open.')]\n",
      "rag_pipeline completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5b12d11e524219b217104f3899dcc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_prompt completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7b40cf330346379c7432e6147846a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate_prompts completed\n",
      "Dataset({\n",
      "    features: ['prompt_key', 'prompt', 'question', 'answer', 'ground_truth', 'answer_correctness', 'faithfulness', 'answer_relevancy', 'context_precision', 'context_recall', 'config'],\n",
      "    num_rows: 1\n",
      "})\n",
      "The average correctness results have been saved to 'rag_average_correctness.csv'\n",
      "rag_get_retriever initiated\n",
      "rag_get_retriever completed\n",
      "prompt_template You are a helpful assistant. Answer any questions solely based on the context provided below. \n",
      "If the provided context does not have the relevant facts to answer the question, say \"I don't know.\"\n",
      "\n",
      "<context>\n",
      "{context}\n",
      "</context>\n",
      "\n",
      "testing retriever\n",
      " [Document(metadata={'source': 'https://raw.githubusercontent.com/ashwinaravind/ashwinaravind.github.io/refs/heads/main/thevanishingtown'}, page_content='A young woman.\\n\\n---\\n\\n## Chunk 3: The Locket’s Secret\\n\\nClara awoke to a sound—footsteps. Her eyes shot open. In the dim light, she saw a man standing at the door, silhouetted against the night. His green eyes gleamed in the shadows.\\n\\n“Who are you?” she stammered, clutching her locket.\\n\\n“I could ask you the same,” the man replied, stepping forward. “This is my family’s cabin. What are you doing here?”'), Document(metadata={'source': 'https://raw.githubusercontent.com/ashwinaravind/ashwinaravind.github.io/refs/heads/main/thevanishingtown'}, page_content='“Who are you?” she stammered, clutching her locket.\\n\\n“I could ask you the same,” the man replied, stepping forward. “This is my family’s cabin. What are you doing here?”\\n\\nClara’s mind raced. Family? She looked him over. There was something familiar about him, though she was certain they had never met. But those eyes—they felt as though they belonged in her memories, as if they had been watching her all her life.'), Document(metadata={'source': 'https://raw.githubusercontent.com/ashwinaravind/ashwinaravind.github.io/refs/heads/main/thevanishingtown'}, page_content='Just before dusk, Clara stumbled upon a small clearing. In the center stood a dilapidated cabin. Exhausted and with nowhere else to go, she approached. The door creaked open with the softest push, revealing a dusty, abandoned space. She slumped down against the cold stone hearth, staring blankly at the flickering fire she had somehow managed to start.')]\n",
      "rag_pipeline completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': \"Clara is a young woman who wakes up in a cabin and has a mysterious encounter with a man who claims the cabin is his family's. She feels a sense of familiarity with him, despite believing they have never met before.\",\n",
       " 'context': 'A young woman.\\n\\n---\\n\\n## Chunk 3: The Locket’s Secret\\n\\nClara awoke to a sound—footsteps. Her eyes shot open. In the dim light, she saw a man standing at the door, silhouetted against the night. His green eyes gleamed in the shadows.\\n\\n“Who are you?” she stammered, clutching her locket.\\n\\n“I could ask you the same,” the man replied, stepping forward. “This is my family’s cabin. What are you doing here?”\\n“Who are you?” she stammered, clutching her locket.\\n\\n“I could ask you the same,” the man replied, stepping forward. “This is my family’s cabin. What are you doing here?”\\n\\nClara’s mind raced. Family? She looked him over. There was something familiar about him, though she was certain they had never met. But those eyes—they felt as though they belonged in her memories, as if they had been watching her all her life.\\nJust before dusk, Clara stumbled upon a small clearing. In the center stood a dilapidated cabin. Exhausted and with nowhere else to go, she approached. The door creaked open with the softest push, revealing a dusty, abandoned space. She slumped down against the cold stone hearth, staring blankly at the flickering fire she had somehow managed to start.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how to use\n",
    "gen=SystemPromptGenerator(GenerationOptionsConfig.from_yaml(\"config.yaml\"),RAGASEvaluator)\n",
    "prompt_key, prompt, max_average_correctness, config=gen.optimize()\n",
    "best_config_py_model=GenerationConfig(**config)\n",
    "best_gen=gen._create_pipeline(best_config_py_model,sample_retriever())\n",
    "best_gen.invoke(\"Who is Clara?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
