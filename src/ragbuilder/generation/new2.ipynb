{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Dict, Any\n",
    "from enum import Enum\n",
    "import importlib\n",
    "\n",
    "# Step 1: Lazy Loading Helper Function\n",
    "def lazy_load(module_name: str, class_name: str):\n",
    "    try:\n",
    "        # Dynamically import the module\n",
    "        module = importlib.import_module(module_name)\n",
    "        # Get the class from the module\n",
    "        return getattr(module, class_name)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading {class_name} from module {module_name}: {e}\")\n",
    "\n",
    "# Step 2: Enum Class for LLM Types\n",
    "class LLM(str, Enum):\n",
    "    OPENAI = \"openai\"\n",
    "    AZURE_OPENAI = \"azure_openai\"\n",
    "    HUGGINGFACE = \"huggingface\"\n",
    "    OLLAMA = \"ollama\"\n",
    "    COHERE = \"cohere\"\n",
    "    VERTEXAI = \"vertexai\"\n",
    "    BEDROCK = \"bedrock\"\n",
    "    JINA = \"jina\"\n",
    "    CUSTOM = \"custom\"\n",
    "\n",
    "# Step 3: Map LLM Types to Lazy-loaded Embedding Classes\n",
    "LLM_MAP = {\n",
    "    LLM.OPENAI: lazy_load(\"langchain_openai\", \"ChatOpenAI\"),\n",
    "    LLM.AZURE_OPENAI: lazy_load(\"langchain_openai\", \"AzureChatOpenAI\"),\n",
    "}\n",
    "\n",
    "# Step 4: Define the LLM Configuration Model\n",
    "class LLMConfig(BaseModel):\n",
    "    model_config = {\"protected_namespaces\": ()}\n",
    "    \n",
    "    type: LLM  # Enum to specify the LLM\n",
    "    model_kwargs: Optional[Dict[str, Any]] = Field(default_factory=dict, description=\"Model-specific parameters like model name/type\")\n",
    "    custom_class: Optional[str] = None  # Optional: If using a custom class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "from langchain.pydantic_v1 import Field, BaseModel\n",
    "\n",
    "\n",
    "class BaseConfig(BaseModel):\n",
    "    \"\"\"Base configuration shared across all RAG modules\"\"\"\n",
    "    # input_source: Union[str, List[str]] = Field(..., description=\"File path, directory path, or URL for input data\")\n",
    "    # test_dataset: str = Field(..., description=\"Path to CSV file containing test questions\")\n",
    "    \n",
    "    @classmethod\n",
    "    def from_yaml(cls, file_path: str) -> \"GenerationOptionsConfig\":\n",
    "        with open(file_path, \"r\") as yaml_file:\n",
    "            config = yaml.safe_load(yaml_file)\n",
    "        return cls(**config[\"Generation\"])\n",
    "\n",
    "    def to_yaml(self, file_path: str) -> None:\n",
    "        \"\"\"Save configuration to a YAML file.\"\"\"\n",
    "        with open(file_path, 'w') as file:\n",
    "            yaml.dump(self.model_dump(), file)\n",
    "\n",
    "# Step 2: Define Pydantic Model for Individual LLM Configuration\n",
    "class GenerationConfig(BaseConfig):\n",
    "    type: LLM  # Specifies the LLM type\n",
    "    model_kwargs: Optional[Dict[str, Any]] = Field(default_factory=dict, description=\"Model-specific parameters\")\n",
    "    prompt_template: Optional[str] = None\n",
    "\n",
    "# Step 3: Define Pydantic Model for Overall Generation Configuration\n",
    "class GenerationOptionsConfig(BaseConfig):\n",
    "    llms: List[GenerationConfig]  # List of LLM configurations\n",
    "    prompt_template_path: Optional[str] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashwinaravind/.pyenv/versions/3.12.5/lib/python3.12/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_kwargs\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "import requests\n",
    "from ragbuilder.generation.config import PromptTemplate\n",
    "import pandas as pd\n",
    "def load_prompts(file_name: str = \"rag_prompts.yaml\", url: str= os.getenv(\"RAG_PROMPT_URL\"),read_local: bool = False):\n",
    "    \"\"\"\n",
    "    Load YAML prompts either from a local file or an online source.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): Name of the YAML file. Defaults to \"rag_prompts.yaml\".\n",
    "        read_local (bool): If True, read from a local file. Otherwise, fetch from an online URL.\n",
    "\n",
    "    Returns:\n",
    "        List[PromptTemplate]: A list of PromptTemplate objects.\n",
    "    \"\"\"\n",
    "    yaml_content = None\n",
    "\n",
    "    if read_local:\n",
    "        # Attempt to read from the local file\n",
    "        if os.path.exists(file_name):\n",
    "            print(f\"Loading prompts from local file: {file_name}\")\n",
    "            with open(file_name, 'r') as f:\n",
    "                yaml_content = f.read()\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Local file not found: {file_name}\")\n",
    "    else:\n",
    "        # Attempt to fetch from an online source\n",
    "        print(f\"Fetching prompts from online file: {url}\")\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Raise an HTTP error for bad responses\n",
    "            yaml_content = response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise RuntimeError(f\"Failed to load prompts from URL {url}: {e}\")\n",
    "\n",
    "    # Parse the YAML content\n",
    "    try:\n",
    "        prompts_data = yaml.safe_load(yaml_content)\n",
    "    except yaml.YAMLError as e:\n",
    "        raise ValueError(f\"Failed to parse YAML content: {e}\")\n",
    "\n",
    "    # Convert YAML entries into PromptTemplate objects\n",
    "    prompts = [\n",
    "        PromptTemplate(name=entry['name'], template=entry['template'])\n",
    "        for entry in prompts_data\n",
    "    ]\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from ragbuilder.generation.evaluation import RAGASEvaluator\n",
    "def sample_retriever():\n",
    "    print(\"rag_get_retriever initiated\")\n",
    "    try:\n",
    "        def format_docs(docs):\n",
    "            return \"\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "        # LLM setup\n",
    "        llm = AzureChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "        # Document loader\n",
    "        loader = WebBaseLoader(\"https://raw.githubusercontent.com/ashwinaravind/ashwinaravind.github.io/refs/heads/main/thevanishingtown\")\n",
    "        docs = loader.load()\n",
    "\n",
    "        # Embedding model\n",
    "        embedding = AzureOpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "        # Text splitting and embedding storage\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "        splits = splitter.split_documents(docs)\n",
    "\n",
    "        # Initialize Chroma database\n",
    "        c = Chroma.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=embedding,\n",
    "            collection_name=\"testindex-ragbuilder-retreiver\",\n",
    "            client_settings=chromadb.config.Settings(allow_reset=True),\n",
    "        )\n",
    "\n",
    "        # Retriever setup\n",
    "        retriever = c.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "        ensemble_retriever = EnsembleRetriever(retrievers=[retriever])\n",
    "        print(\"rag_get_retriever completed\")\n",
    "        return ensemble_retriever\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "# sample_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Load YAML File and Parse Configurations\n",
    "from typing import List, Dict, Type\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "class SystemPromptGenerator:\n",
    "    def __init__(self, config: GenerationOptionsConfig, evaluator_class: Type):\n",
    "        self.llms = []  # List to store instantiated LLMs\n",
    "        self.config = config\n",
    "        print(\"printing config\",config)\n",
    "        # self.evaluator = evaluator_class() \n",
    "        self.retriever=sample_retriever\n",
    "        if config.prompt_template_path:\n",
    "            self.prompt_templates = load_prompts(config.prompt_template_path)\n",
    "        else:\n",
    "            self.prompt_templates = load_prompts()\n",
    "        # for llm_config in config.llms:\n",
    "        #     llm_class = LLM_MAP[llm_config.type]  # Get the corresponding LLM class)\n",
    "    def _build_trial_config(self) -> List[GenerationConfig]:\n",
    "            \"\"\"\n",
    "            Build a list of GenerationConfig objects from the provided GenerationOptionsConfig.\n",
    "\n",
    "            Args:\n",
    "                options_config (GenerationOptionsConfig): The input configuration for trial generation.\n",
    "\n",
    "            Returns:\n",
    "                List[GenerationConfig]: A list of generated configurations for trials.\n",
    "            \"\"\"\n",
    "            trial_configs = []\n",
    "            for llm_config in self.config.llms:\n",
    "                # llm_class = LLM_MAP[llm_config.type]  # Get the corresponding LLM class)\n",
    "                llm_instance = LLMConfig(type=llm_config.type, model_kwargs=llm_config.model_kwargs)\n",
    "                llm_class = LLM_MAP[llm_config.type]\n",
    "                # Step 8: Instantiate the Model with the Configured Parameters\n",
    "                llm = llm_class(**llm_config.model_kwargs)\n",
    "                # print(llm_config.type,llm_config.model_kwargs,llm.invoke(\"what is the capital of France?\"))\n",
    "                # print(self.prompt_templates)\n",
    "                for prompt_template in self.prompt_templates:\n",
    "                    trial_config = GenerationConfig(\n",
    "                        type=llm_config.type,  # Pass the LLMConfig instance here\n",
    "                        model_kwargs=llm_config.model_kwargs,\n",
    "                        # evaluator=self.evaluator,\n",
    "                        # retriever=self.retriever,\n",
    "                        # eval_data_set_path=self.config.eval_data_set_path,\n",
    "                        prompt_template=prompt_template.template,\n",
    "                        # read_local_only=self.config.read_local_only,\n",
    "                        )\n",
    "                    # res=self._create_pipeline(trial_config,self.retriever()).invoke(\"Who is Clara?\")\n",
    "                    # print(res)\n",
    "                    trial_configs.append(trial_config)\n",
    "                    break\n",
    "\n",
    "                # print(trial_config)\n",
    "                # trial_configs.append(trial_config)\n",
    "\n",
    "            # print(trial_configs)\n",
    "            # for llm_config in options_config.llms:\n",
    "            #     print(llm_config)\n",
    "            #     trial_config = GenerationConfig(\n",
    "            #         llm_type=llm_config.type,\n",
    "            #         llm_model_kwargs=llm_config.model_kwargs,\n",
    "            #         evaluator=options_config.evaluator,\n",
    "            #         retriever=options_config.retriever,\n",
    "            #         eval_data_set_path=options_config.eval_data_set_path,\n",
    "            #         prompt_template_path=options_config.prompt_template_path,\n",
    "            #         read_local_only=options_config.read_local_only,\n",
    "            #     )\n",
    "            #     trial_configs.append(trial_config)\n",
    "            return trial_configs\n",
    "    def _create_pipeline(self, trial_config: GenerationConfig, retriever: RunnableParallel):\n",
    "        try:\n",
    "            def format_docs(docs):\n",
    "                return \"\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "            # Prompt setup\n",
    "            llm_class = LLM_MAP[trial_config.type]\n",
    "                # Step 8: Instantiate the Model with the Configured Parameters\n",
    "            llm = llm_class(**trial_config.model_kwargs)\n",
    "            prompt_template = trial_config.prompt_template\n",
    "            print('prompt_template',prompt_template)\n",
    "            print(\"testing retriever\\n\",retriever.invoke(\"Who is Clara?\"))\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\", prompt_template),\n",
    "                    (\"user\", \"{question}\"),\n",
    "                    MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # RAG Chain setup\n",
    "            rag_chain = (\n",
    "                RunnableParallel(context=retriever, question=RunnablePassthrough())\n",
    "                .assign(context=itemgetter(\"context\") | RunnableLambda(format_docs))\n",
    "                .assign(answer=prompt | llm | StrOutputParser())\n",
    "                .pick([\"answer\", \"context\"])\n",
    "            )\n",
    "            print(\"rag_pipeline completed\")\n",
    "            return rag_chain\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    def optimize(self):\n",
    "        trial_configs = self._build_trial_config()\n",
    "        print(\"trial_configs\",trial_configs)\n",
    "        pipeline=None\n",
    "        results = {}\n",
    "        evaluator = RAGASEvaluator()\n",
    "        evaldataset=evaluator.get_eval_dataset('/Users/ashwinaravind/Desktop/kruxgitrepo/ragbuilder/gensimtest.csv')\n",
    "        for trial_config in trial_configs:\n",
    "            pipeline = self._create_pipeline(trial_config,self.retriever())\n",
    "            for entry in evaldataset:\n",
    "                question = entry.get(\"question\", \"\")\n",
    "                result=pipeline.invoke(question)\n",
    "                results[trial_config.prompt_template] = []\n",
    "                results[trial_config.prompt_template].append({\n",
    "                        \"prompt_key\": trial_config.prompt_template,\n",
    "                        \"prompt\": trial_config.prompt_template,\n",
    "                        \"question\": question,\n",
    "                        \"answer\": result.get(\"answer\", \"Error\"),\n",
    "                        \"context\": result.get(\"context\", \"Error\"),\n",
    "                        \"ground_truth\": entry.get(\"ground_truth\", \"\"),\n",
    "                    })\n",
    "                break\n",
    "        # final_results=self.calculate_metrics()\n",
    "        return results\n",
    "    def calculate_metrics(self, result):\n",
    "        # Convert the results to a pandas DataFrame\n",
    "        results_df = result.to_pandas()\n",
    "        \n",
    "        # Calculate average correctness per prompt key\n",
    "        average_correctness = results_df.groupby(['prompt_key','prompt'])['answer_correctness'].mean().reset_index()\n",
    "        average_correctness.columns = ['prompt_key', \"prompt\", 'average_correctness']\n",
    "        \n",
    "        # Save the average correctness to a CSV file\n",
    "        average_correctness.to_csv('rag_average_correctness.csv', index=False)\n",
    "        print(\"The average correctness results have been saved to 'rag_average_correctness.csv'\")\n",
    "        \n",
    "        # Find the row with the highest average correctness\n",
    "        best_prompt_row = average_correctness.loc[average_correctness['average_correctness'].idxmax()]\n",
    "        \n",
    "        # Extract prompt_key, prompt, and average_correctness\n",
    "        prompt_key = best_prompt_row['prompt_key']\n",
    "        prompt = best_prompt_row['prompt']\n",
    "        max_average_correctness = best_prompt_row['average_correctness']\n",
    "        \n",
    "        # return prompt_key, prompt, max_average_correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing config llms=[GenerationConfig(type=<LLM.AZURE_OPENAI: 'azure_openai'>, model_kwargs={'model_name': 'gpt-4o-mini', 'temperature': 0.6}, prompt_template=None)] prompt_template_path=None\n",
      "Fetching prompts from online file: https://raw.githubusercontent.com/ashwinaravind/rag_prompts/refs/heads/main/rag_prompts.yml\n",
      "trial_configs [GenerationConfig(type=<LLM.AZURE_OPENAI: 'azure_openai'>, model_kwargs={'model_name': 'gpt-4o-mini', 'temperature': 0.6}, prompt_template='You are a helpful assistant. Answer any questions solely based on the context provided below. \\nIf the provided context does not have the relevant facts to answer the question, say \"I don\\'t know.\"\\n\\n<context>\\n{context}\\n</context>\\n')]\n",
      "RAGASEvaluator initiated\n",
      "rag_get_retriever initiated\n",
      "rag_get_retriever completed\n",
      "prompt_template You are a helpful assistant. Answer any questions solely based on the context provided below. \n",
      "If the provided context does not have the relevant facts to answer the question, say \"I don't know.\"\n",
      "\n",
      "<context>\n",
      "{context}\n",
      "</context>\n",
      "\n",
      "testing retriever\n",
      " [Document(metadata={'source': 'https://raw.githubusercontent.com/ashwinaravind/ashwinaravind.github.io/refs/heads/main/thevanishingtown'}, page_content='A young woman.\\n\\n---\\n\\n## Chunk 3: The Locket’s Secret\\n\\nClara awoke to a sound—footsteps. Her eyes shot open. In the dim light, she saw a man standing at the door, silhouetted against the night. His green eyes gleamed in the shadows.\\n\\n“Who are you?” she stammered, clutching her locket.\\n\\n“I could ask you the same,” the man replied, stepping forward. “This is my family’s cabin. What are you doing here?”'), Document(metadata={'source': 'https://raw.githubusercontent.com/ashwinaravind/ashwinaravind.github.io/refs/heads/main/thevanishingtown'}, page_content='“Who are you?” she stammered, clutching her locket.\\n\\n“I could ask you the same,” the man replied, stepping forward. “This is my family’s cabin. What are you doing here?”\\n\\nClara’s mind raced. Family? She looked him over. There was something familiar about him, though she was certain they had never met. But those eyes—they felt as though they belonged in her memories, as if they had been watching her all her life.')]\n",
      "rag_pipeline completed\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SystemPromptGenerator.calculate_metrics() missing 1 required positional argument: 'result'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m gen\u001b[38;5;241m=\u001b[39mSystemPromptGenerator(GenerationOptionsConfig\u001b[38;5;241m.\u001b[39mfrom_yaml(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m),RAGASEvaluator)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[9], line 129\u001b[0m, in \u001b[0;36mSystemPromptGenerator.optimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m         results[trial_config\u001b[38;5;241m.\u001b[39mprompt_template]\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    121\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: trial_config\u001b[38;5;241m.\u001b[39mprompt_template,\n\u001b[1;32m    122\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: trial_config\u001b[38;5;241m.\u001b[39mprompt_template,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground_truth\u001b[39m\u001b[38;5;124m\"\u001b[39m: entry\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground_truth\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    127\u001b[0m             })\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m final_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[0;31mTypeError\u001b[0m: SystemPromptGenerator.calculate_metrics() missing 1 required positional argument: 'result'"
     ]
    }
   ],
   "source": [
    "gen=SystemPromptGenerator(GenerationOptionsConfig.from_yaml(\"config.yaml\"),RAGASEvaluator)\n",
    "\n",
    "print(gen.optimize())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
